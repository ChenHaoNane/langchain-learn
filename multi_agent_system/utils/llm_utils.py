"""
LLMå·¥å…·ç±» - åŒ…å«LLMåˆå§‹åŒ–å’Œè°ƒç”¨ç›¸å…³åŠŸèƒ½
"""

import time
import sys
import os
import asyncio
from typing import Any, Dict, Optional, List, Tuple, Callable
from concurrent.futures import ThreadPoolExecutor

from langchain_core.messages import AIMessage, HumanMessage
from langchain_openai import ChatOpenAI

from multi_agent_system.config.settings import (
    API_TIMEOUT, 
    DEFAULT_MODEL, 
    DEFAULT_TEMPERATURE,
    MAX_RETRIES,
    RETRY_DELAY,
    ENABLE_CACHING,
    MAX_PARALLEL_CALLS
)

class LLMManager:
    """LLMç®¡ç†å™¨ï¼Œå¤„ç†æ¨¡å‹åˆå§‹åŒ–å’Œè°ƒç”¨"""
    
    _instance = None
    
    def __new__(cls):
        """å•ä¾‹æ¨¡å¼"""
        if cls._instance is None:
            cls._instance = super(LLMManager, cls).__new__(cls)
            cls._instance.initialized = False
        return cls._instance
    
    def __init__(self):
        """åˆå§‹åŒ–LLMç®¡ç†å™¨"""
        if not self.initialized:
            self.response_cache = {}  # ç®€å•çš„å“åº”ç¼“å­˜
            self.executor = ThreadPoolExecutor(max_workers=MAX_PARALLEL_CALLS)  # çº¿ç¨‹æ± 
            self.initialize_llm()
            self.initialized = True
    
    def initialize_llm(self, model: str = DEFAULT_MODEL, temperature: float = DEFAULT_TEMPERATURE) -> None:
        """åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹
        
        Args:
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦ç³»æ•°
        """
        try:
            # æ·»åŠ ç¼“å­˜æ”¯æŒ
            cache_options = {}
            if ENABLE_CACHING:
                try:
                    from langchain.globals import set_llm_cache
                    from langchain.cache import InMemoryCache
                    set_llm_cache(InMemoryCache())
                    print("âœ… å·²å¯ç”¨LLMå“åº”ç¼“å­˜")
                except ImportError:
                    print("âš ï¸ æ— æ³•å¯¼å…¥ç¼“å­˜æ¨¡å—ï¼Œå°†ç¦ç”¨ç¼“å­˜")
            
            self.llm = ChatOpenAI(
                model=model,
                temperature=temperature,
                request_timeout=API_TIMEOUT,
                **cache_options
            )
            print(f"âœ… æˆåŠŸåˆå§‹åŒ–æ¨¡å‹: {model}")
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–æ¨¡å‹æ—¶å‡ºé”™: {e}")
            print("è¯·ç¡®ä¿æ‚¨å·²ç»è®¾ç½®äº†æ­£ç¡®çš„APIå¯†é’¥å’Œæ¨¡å‹åç§°ã€‚")
            sys.exit(1)
    
    def safe_call(self, prompt: str, retries: int = MAX_RETRIES, delay: int = RETRY_DELAY) -> AIMessage:
        """å®‰å…¨è°ƒç”¨LLMï¼Œå¸¦æœ‰é‡è¯•æœºåˆ¶å’Œç¼“å­˜
        
        Args:
            prompt: æç¤ºæ–‡æœ¬
            retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            delay: é‡è¯•é—´éš”(ç§’)
        
        Returns:
            AIå“åº”æ¶ˆæ¯
        """
        # æ£€æŸ¥ç¼“å­˜
        if ENABLE_CACHING and prompt in self.response_cache:
            print("ğŸ”„ ä½¿ç”¨ç¼“å­˜çš„å“åº”...")
            return self.response_cache[prompt]
            
        for attempt in range(retries):
            try:
                response = self.llm.invoke([HumanMessage(content=prompt)])
                
                # æ·»åŠ åˆ°ç¼“å­˜
                if ENABLE_CACHING:
                    self.response_cache[prompt] = response
                    
                return response
            except Exception as e:
                if attempt < retries - 1:
                    print(f"APIè°ƒç”¨å¤±è´¥ï¼Œæ­£åœ¨é‡è¯• ({attempt+1}/{retries}): {e}")
                    time.sleep(delay)  # ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•
                else:
                    print(f"APIè°ƒç”¨å¤±è´¥ï¼Œè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
                    # è¿”å›ä¸€ä¸ªæ¨¡æ‹Ÿçš„å›åº”
                    return AIMessage(content="APIè°ƒç”¨å¤±è´¥ï¼Œæ— æ³•è·å–å›åº”ã€‚è¯·ç¨åå†è¯•ã€‚")
    
    async def async_call(self, prompt: str, retries: int = MAX_RETRIES, delay: int = RETRY_DELAY) -> AIMessage:
        """å¼‚æ­¥è°ƒç”¨LLM
        
        Args:
            prompt: æç¤ºæ–‡æœ¬
            retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            delay: é‡è¯•é—´éš”(ç§’)
            
        Returns:
            AIå“åº”æ¶ˆæ¯
        """
        # åˆ›å»ºä¸€ä¸ªå¯ä»¥åœ¨äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œçš„å‡½æ•°
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self.executor, 
            lambda: self.safe_call(prompt, retries, delay)
        )
    
    async def parallel_calls(self, prompts: List[str]) -> List[AIMessage]:
        """å¹¶è¡Œè°ƒç”¨LLMå¤„ç†å¤šä¸ªæç¤º
        
        Args:
            prompts: æç¤ºæ–‡æœ¬åˆ—è¡¨
            
        Returns:
            AIå“åº”æ¶ˆæ¯åˆ—è¡¨
        """
        tasks = [self.async_call(prompt) for prompt in prompts]
        return await asyncio.gather(*tasks)
        
    def batch_process(self, prompts: List[str]) -> List[AIMessage]:
        """æ‰¹é‡å¤„ç†å¤šä¸ªæç¤ºï¼ˆåŒæ­¥æ¥å£ï¼‰
        
        Args:
            prompts: æç¤ºæ–‡æœ¬åˆ—è¡¨
            
        Returns:
            AIå“åº”æ¶ˆæ¯åˆ—è¡¨
        """
        # ä¸ºæ²¡æœ‰äº‹ä»¶å¾ªç¯çš„ç¯å¢ƒåˆ›å»ºä¸€ä¸ªæ–°çš„äº‹ä»¶å¾ªç¯
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
        return loop.run_until_complete(self.parallel_calls(prompts))
    
    def change_model(self, model: str, temperature: Optional[float] = None) -> None:
        """æ›´æ”¹ä½¿ç”¨çš„æ¨¡å‹
        
        Args:
            model: æ–°çš„æ¨¡å‹åç§°
            temperature: æ–°çš„æ¸©åº¦ç³»æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä¿æŒä¸å˜
        """
        try:
            temp = temperature if temperature is not None else self.llm.temperature
            self.initialize_llm(model=model, temperature=temp)
        except Exception as e:
            print(f"æ›´æ”¹æ¨¡å‹å¤±è´¥: {e}")

# åˆ›å»ºå…¨å±€å®ä¾‹
llm_manager = LLMManager()

# å¯¼å‡ºä¾¿æ·å‡½æ•°
def safe_llm_call(prompt: str, retries: int = MAX_RETRIES, delay: int = RETRY_DELAY) -> AIMessage:
    """å®‰å…¨è°ƒç”¨LLMçš„ä¾¿æ·å‡½æ•°
    
    Args:
        prompt: æç¤ºæ–‡æœ¬
        retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        delay: é‡è¯•é—´éš”(ç§’)
    
    Returns:
        AIå“åº”æ¶ˆæ¯
    """
    return llm_manager.safe_call(prompt, retries, delay)

def batch_llm_calls(prompts: List[str]) -> List[AIMessage]:
    """æ‰¹é‡å¹¶è¡Œè°ƒç”¨LLMçš„ä¾¿æ·å‡½æ•°
    
    Args:
        prompts: æç¤ºæ–‡æœ¬åˆ—è¡¨
    
    Returns:
        AIå“åº”æ¶ˆæ¯åˆ—è¡¨
    """
    return llm_manager.batch_process(prompts)

def get_llm_manager() -> LLMManager:
    """
    è·å–LLMç®¡ç†å™¨å®ä¾‹
    
    Returns:
        LLMManager: LLMç®¡ç†å™¨å®ä¾‹
    """
    return llm_manager 